---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Group Debugging Squad
execute:
  echo: false
format:
  html:
    theme:
      - minty
      - css/web.scss
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto
    monofont: JetBrainsMono-Regular
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Declaration of Authorship {.unnumbered .unlisted}

We, [The Debugging Squad], confirm that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date:

Student Numbers: 
1. Burhan (23116889)
2. Viktoria Pues (23116898)
3. Yicong Li (23219797)
4. Victoria chen (23233478)
5. Hsu-Kang Sheng (23229993)

## Brief Group Reflection

| What Went Well | What Was Challenging |
| -------------- | -------------------- |
| A              | B                    |
| C              | D                    |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

```{=html}
<style type="text/css">
.duedate {
  border: dotted 2px red; 
  background-color: rgb(255, 235, 235);
  height: 50px;
  line-height: 50px;
  margin-left: 40px;
  margin-right: 40px
  margin-top: 10px;
  margin-bottom: 10px;
  color: rgb(150,100,100);
  text-align: center;
}
</style>
```

{{< pagebreak >}}

# Response to Questions

```{python}
import os
import pandas as pd
```

```{python}
#| jupyter: {source_hidden: true}
host = 'https://orca.casa.ucl.ac.uk'
path = '~jreades/data'
file = '2022-09-10-listings.csv.gz'
url  = f'{host}/{path}/{file}'

if os.path.exists(file):
  df = pd.read_csv(file, compression='gzip', low_memory=False)
else: 
  df = pd.read_csv(url, compression='gzip', low_memory=False)
  df.to_csv(file)
```

## 1. Who collected the data?

The data for the Inside Airbnb Project is collected and managed by the project itself. The data utilizes public information compiled from the Airbnb website including the availability calendar for 365 days in the future, and the reviews for each listing. Data is verified, cleansed, analyzed, and aggregated. Inside Airbnb was founded by Murray Cox, an artist, activist and technologist who conceived the project, compiled and analyzed the data, and built the site. John Morris, a designer and artist, designed and directed the user experience. Apart from these two, Taylor Higgins works to build and organize the data and activist communities of Inside Airbnb.(http://insideairbnb.com/about/).

The project also has a process for handling archived data requests, where requests for larger amounts of data, historical data, or data for uses outside the project's mission may be assessed and, in some cases, refused or subject to payment. 

::: {.duedate}

( 2 points; Answer due Week 7 )

:::


An inline citation: As discussed on @insideairbnb, there are many...

A parenthetical citation: There are many ways to research Airbnb [see, for example, @insideairbnb]... 

## 2. Why did they collect it?

Founded in 2008, Airbnb is a company providing a peer to peer platform for short-term rental accommodations. Private and commercial entities can offer whole accommodations or individual rooms for rent through the website or app. Today, Airbnb has over 4 million active hosts (https://news.airbnb.com/about-us/).

Inside Airbnb is a mission-driven activist project with the objective to make information about the impact of Airbnb on local communities available. The objective is to "provide data that quantifies the impact of short-term rentals on housing and residential communities, as well as create a platform to support advocacy for policies to protect our cities from the impacts of short-term rentals" (http://insideairbnb.com/data-policies).

The main purpose is not only to quantify these impacts, but also to build a platform for advocating for policies that safeguard cities from the negative repercussions of short-term rentals. Inside Airbnb data is used as a tool for housing and community activists, journalists, residents, and local administration to better understand, regulate, and manage concerns related to Airbnb and other short-term rental sites.

::: {.duedate}

( 4 points; Answer due Week 7 )

:::

```{python}
print(f"Data frame is {df.shape[0]:,} x {df.shape[1]:,}")
```

```{python}
ax = df.host_listings_count.plot.hist(bins=50);
ax.set_xlim([0,500]);
```

```{python}
## 3. How was the data collected?  

::: {.duedate}

( 5 points; Answer due Week 8 )
The data on Inside Airbnb was collected through a process of web scraping from the Airbnb website. The data on individual Airbnb Listings is extracted from the Airbnb website for a specified search area, e.g. a city. Inside Airbnb has collected data for a range of cities in Europe and North America (92 cities). For Asia and the Pacific, Africa and South America data was collected only for a few major cities (21 cities) (see: http://insideairbnb.com/explore). 

For each listing, Inside Airbnb collects a variety of data points including on the host (e.g. host name, ID, location, description, picture, total listings count), on the location (e.g. neighborhood, longitude and latitude), on the property (e.g. description, picture, property type, how many people it accommodates, number of bathrooms, bedrooms, beds and other amenities), on the price, on the stay (e.g. min and maximum number of nights and availability),and on reviews (e.g. number of reviews, overall satisfaction rating but not individual reviews). 

According to the website, the collected data is verified, cleansed, analyzed and aggregated by Inside Airbnb. (http://insideairbnb.com/data-assumptions/)

:::
```

## 3. How was the data collected?  

::: {.duedate}

( 5 points; Answer due Week 8 )
The data on Inside Airbnb was collected through a process of web scraping from the Airbnb website. The data on individual Airbnb Listings is extracted from the Airbnb website for a specified search area, e.g. a city. Inside Airbnb has collected data for a range of cities in Europe and North America (92 cities). For Asia and the Pacific, Africa and South America data was collected only for a few major cities (21 cities) (see: http://insideairbnb.com/explore). 

For each listing, Inside Airbnb collects a variety of data points including on the host (e.g. host name, ID, location, description, picture, total listings count), on the location (e.g. neighborhood, longitude and latitude), on the property (e.g. description, picture, property type, how many people it accommodates, number of bathrooms, bedrooms, beds and other amenities), on the price, on the stay (e.g. min and maximum number of nights and availability),and on reviews (e.g. number of reviews, overall satisfaction rating but not individual reviews). 

According to the website, the collected data is verified, cleansed, analyzed and aggregated by Inside Airbnb. (http://insideairbnb.com/data-assumptions/)

:::

## 4. How does the method of collection impact the completeness and/or accuracy of its representation of the process it seeks to study, and what wider issues does this raise?

::: {.duedate}

( 11 points; Answer due Week 9 )

Inside Airbnb seeks to provide data to study Airbnb’s impact on residential communities. Web scraping the Airbnb website as a method for data collection means that the dataset shows a snapshot of listings available at a particular time. The Airbnb website is changing continuously as users add, delete or change listings. As illustrated by Cox and Slee (2016), the website can dramatically change from one day to the next. In 2015, Airbnb published data on their operation in New York using a snapshot of November 17, 2015. The data was misleading as they did not show that a  targeted purge of more than 1,000 listings was implemented just before that date. In order to accurately study the impact of Airbnb on local communities, comparing data over time would be beneficial. 

For the London dataset, used in this assignment, the Airbnb website was scraped on two dates 023-09-06 and 2023-09-07. The dataset seems to be fairly complete. NA values are prominent (over 1000 na values) in the columns reviews_per_month, licence, review_scores_rating_bathroom, bedrooms.

Murray, Slee (2016) How Airbnb’s Data hid the Facts in New York City, available here: http://insideairbnb.com/research/how-airbnb-hid-the-facts-in-nyc.

:::

test 

## 5. What ethical considerations does the use of this data raise? 

::: {.duedate}

( 18 points; Answer due {{< var assess.group-date >}} )

:::

## 6. With reference to the data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London? 

::: {.duedate}



( 15 points; Answer due {{< var assess.group-date >}} )

:::

## 7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how *could* this data set be used to inform the regulation of Short-Term Lets (STL) in London? 

::: {.duedate}

( 45 points; Answer due {{< var assess.group-date >}} )

:::

## Sustainable Authorship Tools

Your QMD file should automatically download your BibTeX file. We will then re-run the QMD file to generate the output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References

```{python}
# load all the libraries needed 
import os
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
```

```{python}
import os
import requests
from urllib.parse import urlparse
import gzip
import shutil
import pandas as pd

def cache_data(src: str, dest: str) -> str:
    """
       
    Downloads and caches a remote file locally.
    
    The function sits between the 'read' step of a pandas or geopandas
    data frame and downloading the file from a remote location. The idea
    is that it will save it locally so that you don't need to remember to
    do so yourself. Subsequent re-reads of the file will return instantly
    rather than downloading the entire file for a second or n-th itme.

    We've built in some basic logic around looking at the extension of the 
    destination file and converting it accordingly *once* it is downloaded.
    
    Parameters
    ----------
    src : str
        The remote *source* for the file, any valid URL should work.
    dest : str
        The *destination* location to save the downloaded file.
        
    Returns
    -------
    str
        A string representing the local location of the file.

        
    """
    
    url = urlparse(src)
    fn = os.path.split(url.path)[-1]
    dfn = os.path.join(dest, fn)

    if not os.path.isfile(dfn):
        print(f"{dfn} not found, downloading!")

        path = os.path.split(dest)

        if len(path) >= 1 and path[0] != '':
            os.makedirs(os.path.join(*path), exist_ok=True)

        with open(dfn, "wb") as file:
            response = requests.get(src, stream=True)
            shutil.copyfileobj(response.raw, file)

        print("\tDone downloading...")

    else:
        print(f"Found {dfn} locally!")

    return dfn

# Define the destination directory and source path
ddir = os.path.join('data', 'listings')  # destination directory
spath = 'http://data.insideairbnb.com/united-kingdom/england/london/2023-09-06/data/listings.csv.gz'  # source path

# Use the cache_data function to download and cache the file
cached_file_path = cache_data(spath, ddir)

# Read the cached file into a pandas DataFrame
listings_df = pd.read_csv(cached_file_path)

# Now 'listings_df' contains the DataFrame with the data from the cached CSV file
print('Done.')
```

```{python}
print(listings_df.columns.to_list())
```

```{python}
# select the columns on host and listing type from list above that are of interested in for this question. 
cols = ['id', 'name', 'description', 'host_id', 'host_name', 'host_since', 'host_location', 'host_about','host_neighbourhood','host_listings_count', 'host_total_listings_count', 'host_identity_verified', 'neighbourhood', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'latitude', 'longitude', 'property_type', 'room_type', 'price', 'minimum_nights', 'maximum_nights', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms']
```

```{python}
# delete the df and read in again, only using the selected columns 
del(listings_df)
listings_df1 = pd.read_csv(cached_file_path)[cols]
print(f"Data frame is {listings_df1.shape[0]:,} x {listings_df1.shape[1]}")
print(listings_df1.columns.to_list())
```

```{python}
#identify problematic rows 
listings_df1.isnull().sum(axis=0).sort_values(ascending=False)
```

```{python}
#there are some columns with very high numbers of na. They dont seem relevant for the analysis, so we drop them. 
columns_drop = ['neighbourhood_group_cleansed', 'host_about', 'neighbourhood', 'host_neighbourhood', 'host_location'] 
listings_df1.drop(columns=columns_drop, inplace=True)

#there are a few columns that have exactly 5 na. Looks like these are not complete listings. So we drop them. 
cols_na = ['host_name', 'host_since', 'host_listings_count', 'host_total_listings_count', 'host_identity_verified']
listings_df1.dropna(subset=cols_na, inplace=True)

print(f"Data frame is now {listings_df1.shape[0]:,} x {listings_df1.shape[1]}")
```

```{python}
# the price column has a dollar sign and comma, need to drop comma and dollar sign  
money = ['price']
for m in money:
    print(f"Converting {m}")
    listings_df1[m] = listings_df1[m].str.replace('$','', regex=False).str.replace(',','', regex=False).astype('float')
```

```{python}
# check it worked 
listings_df1.sample(5, random_state=42)[money]
```

```{python}
#check for extremes 
listings_df1[listings_df1['price'] == 0]
```

```{python}
#delete the ones where price is 0 
listings_df1=listings_df1[listings_df1['price'] != 0]
```

```{python}
#there are also some columns that should be numeric. Converting columns into integers.  
ints  = ['id','host_id','host_listings_count','host_total_listings_count',
        'minimum_nights','maximum_nights']
for i in ints:
    print(f"Converting {i}")
    try:
        listings_df1[i] = listings_df1[i].astype('float').astype('int')
    except ValueError as e:
        print("  - !!!Converting to unsigned 16-bit integer!!!")
        df[i] = listings_df1[i].astype('float').astype(pd.UInt16Dtype())
```

```{python}
# room_type, make bouleon
# property_type, make bouleon
listings_df1.room_type.astype('category').memory_usage(deep=True) 
listings_df1.property_type.astype('category').memory_usage(deep=True)
```

```{python}
#categories 
cats = ['property_type','room_type']
listings_df1.sample(5, random_state=42)[cats]
```

```{python}
listings_df1[cats[0]].value_counts()
```

```{python}
#identify the listings with unrealistically high prives 
listings_df1.sample(5, random_state=42)[cats] 
```

```{python}
#drop the rows with unrealistically high prices 

listings_df1 = listings_df1.drop([11248, 39139])
```

```{python}
### desriptive statistics ####

# The mean and median price of airbnb in London is. 
print(f"The mean price is ${listings_df1.price.mean():0.2f}")
print(f"The median price is ${listings_df1.price.median():0.2f}")
print(f"The min price is ${listings_df1.price.min():0.2f}")
print(f"The max price is ${listings_df1.price.max():0.2f}")
print(f"The price standard deviattion is ${listings_df1.price.std():0.2f}")
```

```{python}
# plot price in historam 
listings_df1.price.plot.hist(bins=50)
```

```{python}
#plotting 
listings_df1.plot.scatter(x='longitude', y='latitude', c='price', s=2, cmap='viridis', figsize=(15,10))
```
