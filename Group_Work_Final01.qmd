---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Group Debugging Squad
execute:
  echo: false
format:
  html:
    theme:
      - minty
      - css/web.scss
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto
    monofont: JetBrainsMono-Regular
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Declaration of Authorship {.unnumbered .unlisted}

We, [The Debugging Squad], confirm that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date:

Student Numbers: 
1. Burhan (23116889)
2. Viktoria Pues (23116898)
3. Yicong Li (23219797)
4. Victoria chen (23233478)
5. Hsu-Kang Sheng (23229993)

## Brief Group Reflection

| What Went Well | What Was Challenging |
| -------------- | -------------------- |
| A              | B                    |
| C              | D                    |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

```{=html}
<style type="text/css">
.duedate {
  border: dotted 2px red; 
  background-color: rgb(255, 235, 235);
  height: 50px;
  line-height: 50px;
  margin-left: 40px;
  margin-right: 40px
  margin-top: 10px;
  margin-bottom: 10px;
  color: rgb(150,100,100);
  text-align: center;
}
</style>
```

{{< pagebreak >}}

# Response to Questions

```{python}
import os
import pandas as pd
```

```{python}
host = 'https://orca.casa.ucl.ac.uk'
path = '~jreades/data'
file = '2022-09-10-listings.csv.gz'
url  = f'{host}/{path}/{file}'

if os.path.exists(file):
  df = pd.read_csv(file, compression='gzip', low_memory=False)
else: 
  df = pd.read_csv(url, compression='gzip', low_memory=False)
  df.to_csv(file)
```

## 1. Who collected the data?

The data for the Inside Airbnb Project is collected and managed by the project itself. The data utilizes public information compiled from the Airbnb website including the availability calendar for 365 days in the future, and the reviews for each listing. Data is verified, cleansed, analyzed, and aggregated. Inside Airbnb was founded by Murray Cox, an artist, activist and technologist who conceived the project, compiled and analyzed the data, and built the site. John Morris, a designer and artist, designed and directed the user experience. Apart from these two, Taylor Higgins works to build and organize the data and activist communities of Inside Airbnb.(http://insideairbnb.com/about/).

The project also has a process for handling archived data requests, where requests for larger amounts of data, historical data, or data for uses outside the project's mission may be assessed and, in some cases, refused or subject to payment. 

::: {.duedate}

( 2 points; Answer due Week 7 )

:::


An inline citation: As discussed on @insideairbnb, there are many...

A parenthetical citation: There are many ways to research Airbnb [see, for example, @insideairbnb]... 

## 2. Why did they collect it?

Founded in 2008, Airbnb is a company providing a peer to peer platform for short-term rental accommodations. Private and commercial entities can offer whole accommodations or individual rooms for rent through the website or app. Today, Airbnb has over 4 million active hosts (https://news.airbnb.com/about-us/).

Inside Airbnb is a mission-driven activist project with the objective to make information about the impact of Airbnb on local communities available. The objective is to "provide data that quantifies the impact of short-term rentals on housing and residential communities, as well as create a platform to support advocacy for policies to protect our cities from the impacts of short-term rentals" (http://insideairbnb.com/data-policies).

The main purpose is not only to quantify these impacts, but also to build a platform for advocating for policies that safeguard cities from the negative repercussions of short-term rentals. Inside Airbnb data is used as a tool for housing and community activists, journalists, residents, and local administration to better understand, regulate, and manage concerns related to Airbnb and other short-term rental sites.

::: {.duedate}

( 4 points; Answer due Week 7 )

:::

```{python}
print(f"Data frame is {df.shape[0]:,} x {df.shape[1]:,}")
```

```{python}
ax = df.host_listings_count.plot.hist(bins=50);
ax.set_xlim([0,500]);
```

```{python}
## 3. How was the data collected?  

::: {.duedate}

( 5 points; Answer due Week 8 )
The data on Inside Airbnb was collected through a process of web scraping from the Airbnb website. The data on individual Airbnb Listings is extracted from the Airbnb website for a specified search area, e.g. a city. Inside Airbnb has collected data for a range of cities in Europe and North America (92 cities). For Asia and the Pacific, Africa and South America data was collected only for a few major cities (21 cities) (see: http://insideairbnb.com/explore). 

For each listing, Inside Airbnb collects a variety of data points including on the host (e.g. host name, ID, location, description, picture, total listings count), on the location (e.g. neighborhood, longitude and latitude), on the property (e.g. description, picture, property type, how many people it accommodates, number of bathrooms, bedrooms, beds and other amenities), on the price, on the stay (e.g. min and maximum number of nights and availability),and on reviews (e.g. number of reviews, overall satisfaction rating but not individual reviews). 

According to the website, the collected data is verified, cleansed, analyzed and aggregated by Inside Airbnb. (http://insideairbnb.com/data-assumptions/)

:::
```

## 3. How was the data collected?  

::: {.duedate}

( 5 points; Answer due Week 8 )
The data on Inside Airbnb was collected through a process of web scraping from the Airbnb website. The data on individual Airbnb Listings is extracted from the Airbnb website for a specified search area, e.g. a city. Inside Airbnb has collected data for a range of cities in Europe and North America (92 cities). For Asia and the Pacific, Africa and South America data was collected only for a few major cities (21 cities) (see: http://insideairbnb.com/explore). 

For each listing, Inside Airbnb collects a variety of data points including on the host (e.g. host name, ID, location, description, picture, total listings count), on the location (e.g. neighborhood, longitude and latitude), on the property (e.g. description, picture, property type, how many people it accommodates, number of bathrooms, bedrooms, beds and other amenities), on the price, on the stay (e.g. min and maximum number of nights and availability),and on reviews (e.g. number of reviews, overall satisfaction rating but not individual reviews). 

According to the website, the collected data is verified, cleansed, analyzed and aggregated by Inside Airbnb. (http://insideairbnb.com/data-assumptions/)

:::

## 4. How does the method of collection impact the completeness and/or accuracy of its representation of the process it seeks to study, and what wider issues does this raise?

::: {.duedate}

( 11 points; Answer due Week 9 )

Inside Airbnb seeks to provide data to study Airbnb’s impact on residential communities. Web scraping the Airbnb website as a method for data collection means that the dataset shows a snapshot of listings available at a particular time. The Airbnb website is changing continuously as users add, delete or change listings. As illustrated by Cox and Slee (2016), the website can dramatically change from one day to the next. In 2015, Airbnb published data on their operation in New York using a snapshot of November 17, 2015. The data was misleading as they did not show that a  targeted purge of more than 1,000 listings was implemented just before that date. In order to accurately study the impact of Airbnb on local communities, comparing data over time would be beneficial. 

For the London dataset, used in this assignment, the Airbnb website was scraped on two dates 023-09-06 and 2023-09-07. The dataset seems to be fairly complete. NA values are prominent (over 1000 na values) in the columns reviews_per_month, licence, review_scores_rating_bathroom, bedrooms.

Murray, Slee (2016) How Airbnb’s Data hid the Facts in New York City, available here: http://insideairbnb.com/research/how-airbnb-hid-the-facts-in-nyc.

:::

test 

## 5. What ethical considerations does the use of this data raise? 

::: {.duedate}

( 18 points; Answer due {{< var assess.group-date >}} )

:::

## 6. With reference to the data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London? 

::: {.duedate}



( 15 points; Answer due {{< var assess.group-date >}} )

:::

## 7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how *could* this data set be used to inform the regulation of Short-Term Lets (STL) in London? 

::: {.duedate}

( 45 points; Answer due {{< var assess.group-date >}} )

:::

## Sustainable Authorship Tools

Your QMD file should automatically download your BibTeX file. We will then re-run the QMD file to generate the output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References




# Coding Area




# 6. With reference to the data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London? 

```{python}
# First of all_Data cleaning
```

```{python}
# load all the libraries needed 
import os
import requests
from urllib.parse import urlparse
import gzip
import shutil
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt


def cache_data(src: str, dest: str) -> str:
    """
       
    Downloads and caches a remote file locally.
    
    The function sits between the 'read' step of a pandas or geopandas
    data frame and downloading the file from a remote location. The idea
    is that it will save it locally so that you don't need to remember to
    do so yourself. Subsequent re-reads of the file will return instantly
    rather than downloading the entire file for a second or n-th itme.

    We've built in some basic logic around looking at the extension of the 
    destination file and converting it accordingly *once* it is downloaded.
    
    Parameters
    ----------
    src : str
        The remote *source* for the file, any valid URL should work.
    dest : str
        The *destination* location to save the downloaded file.
        
    Returns
    -------
    str
        A string representing the local location of the file.

        
    """
    
    url = urlparse(src)
    fn = os.path.split(url.path)[-1]
    dfn = os.path.join(dest, fn)

    if not os.path.isfile(dfn):
        print(f"{dfn} not found, downloading!")

        path = os.path.split(dest)

        if len(path) >= 1 and path[0] != '':
            os.makedirs(os.path.join(*path), exist_ok=True)

        with open(dfn, "wb") as file:
            response = requests.get(src, stream=True)
            shutil.copyfileobj(response.raw, file)

        print("\tDone downloading...")

    else:
        print(f"Found {dfn} locally!")

    return dfn

# Define the destination directory and source path
ddir = os.path.join('data', 'listings')  # destination directory
spath = 'http://data.insideairbnb.com/united-kingdom/england/london/2023-09-06/data/listings.csv.gz'  # source path

# Use the cache_data function to download and cache the file
cached_file_path = cache_data(spath, ddir)

# Read the cached file into a pandas DataFrame
listings_df = pd.read_csv(cached_file_path)

# Now 'listings_df' contains the DataFrame with the data from the cached CSV file
print('Done.')
```

```{python}
# Loading borough map

import geopandas as gpd
import matplotlib.pyplot as plt

# Load the London borough boundary shapefile
# gdf = gpd.read_file("C:/Users/avb19/Documents/CASA/FSDS/FSDS_Debugging_Squad/ESRI/London_Borough_Excluding_MHW.shp")

ddir  = os.path.join('data','geo') # destination directory
spath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path

gdf = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )

print(gdf.crs)
print(gdf.head())
print(gdf.columns)

gdf.plot()
```

```{python}
# Data Cleaning
```

```{python}
# View columns
print(listings_df.columns.to_list())
```

```{python}
# Select the columns on host and listing type from list above that are of interested in for this question. 
cols = ['id', 'listing_url', 'name', 'description', 
        'host_id', 'host_name', 'host_since', 'host_location',
        'host_neighbourhood','host_listings_count', 'host_total_listings_count', 'host_identity_verified',
        'neighbourhood', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed',
        'latitude', 'longitude',
        'property_type', 'room_type', 'accommodates', 'bedrooms', 'beds',
        'price', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 
        'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'reviews_per_month',
        'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms']

# print(df_2023_Season4_SC.head()) # Just check
cols.index('calculated_host_listings_count_shared_rooms') # Prints: 34
```

```{python}
# Delete the df and read in again, only using the selected columns 
# del(listings_df)
listings_df1 = pd.read_csv(cached_file_path)[cols]
print(f"Data frame is {listings_df1.shape[0]:,} x {listings_df1.shape[1]}")
print(listings_df1.columns.to_list())
```

```{python}
# Identify problematic rows 
listings_df1.isnull().sum(axis=0).sort_values(ascending=False)
```

```{python}
# There are some columns with very high numbers of na. They dont seem relevant for the analysis, so we drop them. 
columns_drop = ['neighbourhood_group_cleansed', 'neighbourhood', 'host_neighbourhood', 'host_location'] 
listings_df1.drop(columns=columns_drop, inplace=True)
```

```{python}
# There are a few columns that have exactly 5 na. Looks like these are not complete listings. So we drop them. 
cols_na = ['host_name', 'host_since', 'host_listings_count', 'host_total_listings_count', 'host_identity_verified']
listings_df1.dropna(subset=cols_na, inplace=True)
```

```{python}
print(f"Data frame is now {listings_df1.shape[0]:,} x {listings_df1.shape[1]}")
# Prints: Data frame is now 87,941 x 30
```

```{python}
# Just check again 
listings_df1.isnull().sum(axis=0).sort_values(ascending=False)
```

## 6.1 Item 'price' analysis

```{python}
### 6.1.1 Data standardization_Price
money = ['price']
listings_df1.sample(5, random_state=42)[money] # Just check
```

```{python}
# The 'price' column has a dollar sign and comma, need to drop comma and dollar sign  
for m in money:
    print(f"Converting {m}")
    listings_df1[m] = listings_df1[m].str.replace('$','', regex=False).str.replace(',','', regex=False).astype('float')
```

```{python}
# Check it worked 
listings_df1.sample(5, random_state=42)[money]
```

```{python}
# Check for extremes 
listings_df1[listings_df1['price'] == 0]
```

```{python}
#delete the ones where price is 0 
listings_df1=listings_df1[listings_df1['price'] != 0]
```

```{python}
#Just check agian
print(f"Data frame is now {listings_df1.shape[0]:,} x {listings_df1.shape[1]}")
# Prints: Data frame is now 87,938 x 30
```

```{python}
#there are also some columns that should be numeric. Converting columns into integers.  
ints  = ['id', 'host_id', 'host_listings_count', 'host_total_listings_count', 
        'minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights']
for i in ints:
    print(f"Converting {i}")
    try:
        listings_df1[i] = listings_df1[i].astype('float').astype('int')
    except ValueError as e:
        print("  - !!!Converting to unsigned 16-bit integer!!!")
        df[i] = listings_df1[i].astype('float').astype(pd.UInt16Dtype())
```

```{python}
# room_type, make bouleon
# property_type, make bouleon
listings_df1.room_type.astype('category').memory_usage(deep=True) 
listings_df1.property_type.astype('category').memory_usage(deep=True)
```

Item 'price' has a big influence when travellers choose the lisitng as accomodation on Airbnb.

However, the information of 'price' on our dataset is not fully accurate. When we filtered the room rates, we found that there were many listings with prices above 1,500 per night. On the whole, there are a small number of implausibly high prices for individual units that aren't in very expensive neighbourhoods and that these are either erroneous/deliberately incorrect, or represent a price that is not per-night. We can set a special number to filter all rows which price is larger than this number.

```{python}
### desriptive statistics ####

# The mean and median price of airbnb in London is. 
print(f"The mean price is ${listings_df1.price.mean():0.2f}")
print(f"The median price is ${listings_df1.price.median():0.2f}")
print(f"The min price is ${listings_df1.price.min():0.2f}")
print(f"The max price is ${listings_df1.price.max():0.2f}")
print(f"The price standard deviattion is ${listings_df1.price.std():0.2f}")
```

```{python}
listings_df1[listings_df1['price']==80100]['listing_url']
```

```{python}
# convert the column 'price'
ints_price  = ['price']
for i in ints_price:
    print(f"Converting {i}")
    try:
        listings_df1[i] = listings_df1[i].astype('float').astype('int')
    except ValueError as e:
        print("  - !!!Converting to unsigned 16-bit integer!!!")
        df[i] = listings_df1[i].astype('float').astype(pd.UInt16Dtype())
```

```{python}
# Filter too large 'price' number
# set '2000' as special number, check and show the rows which 'price' is larger than '2000'
count_gt_2000 = len(listings_df1[listings_df1['price'] > 2000])
rows_gt_2000 = listings_df1[listings_df1['price'] > 2000]
print(f"The number of row which 'price' is larger than '2000'：{count_gt_2000} rows")

price_review = rows_gt_2000[rows_gt_2000['number_of_reviews_ltm']>0]
print(price_review[['price', 'number_of_reviews_ltm', 'property_type', 'room_type']])
```

For example, we can see that 'number_of_reviews_ltm' of listing '4962' is 28. When we go to url it shows, we found that the price is '50' now. However, the price in the csv is '6000'. (Source: 'https://www.airbnb.com/rooms/6674937') It shows it is necessary to drop 'price' is higher than '1500'. Even so, this is still a rough filter because we still can’t determine whether any of the remaining listings have incorrect prices. This also shows that Airbnb’s statistics are not accurate.

```{python}
listings_df1.iloc[4962]['listing_url']
```

```{python}
listings_df1.drop(listings_df1[listings_df1['price'] >= 1500].index, inplace=True)
listings_df1.shape
```

```{python}
### Plot.histogram ###

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Plot histogram with prices less than 1500
plt.figure(figsize=(10, 6))
price_histogram = listings_df1[listings_df1['price'] < 1500]['price'].plot.hist(bins=200)
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.title('Histogram of Prices Below 1500')
plt.grid(True)
plt.show()
```

```{python}
### Box plot ###

# calculate the price between 5%-95%
percentile_05 = listings_df1['price'].quantile(0.05)
percentile_95 = listings_df1['price'].quantile(0.95)
data = listings_df1[(listings_df1['price'] >= percentile_05) & (listings_df1['price'] <= percentile_95)]['price']

# create a new figure
plt.figure(figsize=(5, 8))

# create a box plot()
plt.boxplot(data, vert=True) 

# Create the x-coordinate of the data points (arranged along the ordinate)
#x_coordinates = np.ones(len(data))
x_coordinates = np.random.normal(1, 0.04, size=len(data))  # Add slight perturbation to avoid overlap

# add points
plt.scatter(x_coordinates, data, color='red', marker='o', s=0.5, label='Data Points', alpha = 0.2)

# Set the style of outliers
flierprops = dict(marker='o', markerfacecolor='blue', markersize=5, linestyle='none', markeredgecolor='black')
whiskerprops = {'linewidth':0.5}
plt.boxplot(data, vert=True, whiskerprops=whiskerprops, flierprops=flierprops)

# Set X-axis tick labels
plt.xticks([1], ['Data'])

# Show legend
plt.legend()

# Label values for minimum, first quartile, median, third quartile, and maximum
plt.text(0.75, min(data), f'Minimum: {min(data):.2f}', va='bottom', ha='center')
plt.text(0.75, np.percentile(data, 25), f'1st Quartile: {np.percentile(data, 25):.2f}', va='bottom', ha='center')
plt.text(0.75, np.median(data), f'Median: {np.median(data):.2f}', va='bottom', ha='center')
plt.text(0.75, np.percentile(data, 75), f'3rd Quartile: {np.percentile(data, 75):.2f}', va='bottom', ha='center')
plt.text(0.75, max(data), f'Maximum: {max(data):.2f}', va='bottom', ha='center')

# show plot
plt.show()
```

When drawing the box plot, I selected 5%-95% of the data for analysis, in order to eliminate the impact of outliers that are too large or too small in the data. Based on the box plot and histogram, we can see that the median of the listing is 106.00, and 50% of the data is concentrated between 65.00-176.00. 

```{python}
### Plot.scatter of price distribution ###

import geopandas as gpd
import matplotlib.pyplot as plt
import pandas as pd
import pysal as p
import mapclassify as mc
import palettable.matplotlib as palmpl
from legendgram import legendgram

# If your gdf is not in WGS 84 (latitude/longitude), and your listings_df1 data is in WGS 84, you need to convert it.
# Ensure listings data is in a GeoDataFrame with the correct CRS.
gdf_listings = gpd.GeoDataFrame(
    listings_df1,
    geometry=gpd.points_from_xy(listings_df1.longitude, listings_df1.latitude),
    crs="EPSG:4326"  # WGS 84
)
# Now convert the listings GeoDataFrame to match the CRS of the borough boundaries GeoDataFrame.
gdf_listings = gdf_listings.to_crs(gdf.crs)

# define price_range
price_range = [(0,50), (50,100), (100,150), (150,200), (200,250), (250,300), (300,float('inf'))]
# create a new column 'price_range', reflect prices by 'price_range' to the corresponding labels 
listings_df1['price_range'] = pd.cut(listings_df1['price'], 
                                       bins=[0, 50, 100, 150, 200, 250, 300, float('inf')],
                                       labels=['(0, 50)', '(50, 100)', '(100, 150)', '(150, 200)', '(200, 250)', '(250, 300)', '300+'],
                                       right=False)

# Now we can plot both on the same Axes object.
fig, ax = plt.subplots(1, 1, figsize=(15, 10))
# Set the color of the boroughs.
gdf.plot(ax=ax, color='none', edgecolor='black', facecolor='black', linewidth=1.5)  
gdf_listings.plot(ax=ax, column='price_range', cmap='RdBu_r', legend=True, 
                 marker='.', markersize=3, zorder=4)

# Set axis labels and title using a specified font, weight, and size
ax.set_xlabel('Easting', fontsize=15)
ax.set_ylabel('Northing', fontsize=15)
ax.set_title('Airbnb Listing Price Distribution in London Borough', 
             fontdict={'fontsize':'20', 'fontweight':'3'})  #provide a title
plt.grid(True)
plt.show()

#listings_df1.drop(columns=['price_range'], inplace=True)
```

Finally, we can draw the spatial distribution map of the price of each listing per night in London. It can be seen from this that the listing prices in inner London, the more economically developed areas such as Camdon, City of London, Westminster, etc, are generally higher. While in outer London, the listing prices are generally lower. This is more in line with our general knowledge。

## 6.2 "Host type" analysis

"Host type" refers to whether the host of listing is a individual host or a commercial host. We identify the "host type" by the item 'calculated_host_listings_count', which means how many listings the host has on Airbnb.(https://rstudio-pubs-static.s3.amazonaws.com/407929_afc5ef0f2ad648389447a6ca3f4a7cd4.html) We define the host who has only one listing is an individual host, and those with two or more are commercial hosts.

It is generally accepted that individual host is more likely to be a local resident, which can be called as 'local host'. Their income can be seen as contributing to the income and economy of the local population. In contrast, hosts with multiple listings are more likely to be running a business, are unlikely to be living in the property, and in violation of most short term rental laws designed to protect residential housing.(http://insideairbnb.com/london) Their incomes add very little to local economic development.

```{python}
### Percentage of individual hosts (local hosts) in the total number of hosts ###

import pandas as pd

# Updated function to calculate the ratio of single listings by neighbourhood
def calculate_host_type(group):
    # Calculate the count of single listings
    single_listings_count = (group['calculated_host_listings_count'] == 1).sum()
    # Calculate the ratio of single listings
    local_host_proportion = single_listings_count / len(group)
    # Return the ratio with the neighbourhood name
    return pd.Series({'local_host_proportion': local_host_proportion})

# Group by neighbourhood and apply the calculation, then reset index
local_host_proportion_df = listings_df1.groupby('neighbourhood_cleansed').apply(calculate_host_type).reset_index()

# Display the resulting DataFrame
local_host_proportion_df
```

```{python}
import matplotlib.pyplot as plt
import geopandas as gpd

# Merge your data with the GeoDataFrame
# Make sure the left and right column names are correct
local_host_proportion_df_gdf = gdf.merge(local_host_proportion_df, left_on='NAME', right_on='neighbourhood_cleansed')

# Plot the map
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
local_host_proportion_df_gdf.plot(column='local_host_proportion', ax=ax, legend=True, cmap='RdBu_r', edgecolor='white', 
                   legend_kwds={'label': "Proportion of local host",
                                'orientation': "vertical",
                                'shrink': 0.7})

# Set axis labels and title using a specified font, weight, and size
ax.set_xlabel('Easting', fontsize=15)
ax.set_ylabel('Northing', fontsize=15)
ax.set_title('Proportion of local host in London Borough', 
             fontdict={'fontsize':'20', 'fontweight':'3'})  #provide a title

# Show the plot
plt.show()
```

The results show how few individual hosts there are in inner London, such as Camdon and Westminster. In the outer London area, the ptoportion of individual hosts is high.

```{python}
???
```

## 6.3 "Occupancy" analysis

"Occupancy" means the days listing is used per year. We can use item "occupancy rate" and "occupancy days per booking" to calculate. The exact calculation process will be shown in detail in Question 7.

???

```{python}
import pandas as pd

# calculate the occupancy day in 2023 year
def calculate_occupancy(df):
    df['occupancy_rate'] = df['number_of_reviews_ltm'] / 12 * 3 / 0.50
    df['occupancy'] = df['occupancy_rate'] * df['minimum_nights']
    return df['occupancy'].sum()

# use groupby() divide into boroughs and apply calculate_occupancy() function
occupancy_by_borough = listings_df1.groupby('neighbourhood_cleansed').apply(calculate_occupancy)

# Convert the result to a DataFrame with the correct column name.
occupancy_by_borough_df = occupancy_by_borough.reset_index(name='occupancy')

print(type(occupancy_by_borough_df))
occupancy_by_borough_df
```

```{python}
import pandas as pd

# Merging the dataframes on 'neighbourhood_cleansed'
merged_occupancy_df = occupancy_by_borough_df.merge(neighborhood_counts, on='neighbourhood_cleansed')
merged_occupancy_df['avarage_occupancy_day'] = merged_occupancy_df['occupancy'] / merged_occupancy_df['total_listings']

merged_occupancy_df
```

```{python}
# Make a Map

# Merge your data with the GeoDataFrame
merged_occupancy_gdf = gdf.merge(merged_occupancy_df, left_on='NAME', right_on='neighbourhood_cleansed')

# Plot the map
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
merged_occupancy_gdf.plot(column='avarage_occupancy_day', ax=ax, legend=True, cmap='RdBu_r', edgecolor='white',
                legend_kwds={'label': "Avarage Occupancy Day",
                             'orientation': "vertical", 
                             'shrink': 0.7})

# Set axis labels and title using a specified font, weight, and size
ax.set_xlabel('Easting', fontsize=15)
ax.set_ylabel('Northing', fontsize=15)
ax.set_title('Avarage Occupancy Day in London Borough', 
             fontdict={'fontsize':'20', 'fontweight':'3'})  #provide a title

# Show the plot
plt.show()
```

The result shows that some boroughs have more occupancy days. It can be explained by different reasons. For inner London such as Camdon, Westminster and city of London, that's because it is more popular and convenient. For outer London such as Richmond upon Thames, the reason maybe its gentle environment.

```{python}
???
```

# 7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how could this data set be used to inform the regulation of Short-Term Lets (STL) in London?




## 7.1 Research Question

Airbnb claims that it contributes to dispersing tourism across the city's Boroughs in London.

(https://news.airbnb.com/en-uk/london-is-open-new-report-shows-airbnb-guests-opt-to-stay-off-the-beaten-track/) 

The research question we pose is how much Airbnb contributes to tourist money being spent locally and how this compares across boroughs in London. 

## 7.2 Approach

To understand the tourist money spent locally related to Airbnb in London we will conduct the following analysis. An overview on assumptions is provided below. 

1.Calculate the occupancy rate for each listing, to know how often it was booked in  a year on average, using the same approach as done in the San Francisco model as presented on the Inside Airbnb website (link). 

2.To assess tourist expenditure that benefits the local community, we assume that it comes from: (a) money spent locally, e.g. in local businesses and(b) money paid to the local airbnb host. We assume that a tourist spends 30 GBP locally a day on average. 

**Money to the local economy for each listing (year 2023) =  tourist local spending + income of single host (if host = local host).**

**Tourist local spending =  occupancy rate * number of nights * number of people * 30GBP**

**Income host =  price * occupancy rate * number of nights**

*Just to confirm: occupancy(number of days in year 2023) = occupancy rate * number of nights*

3.We group listings by borough and aggregate local spending for each borough and divide it by the count of listings in each borough to account for density. 

**Money to local economy for each borough = sum of money to local economy for each listing in that borough / sum of listings in that borough**

4.Money to local economy for each borough = sum of money to local economy for each listing in that borough / sum of listings in that borough

## 7.3 Overview of assumptions

**1.Occupancy Rate:** 

We calculate the occupancy rate based on the number of reviews in the last 12 months and assume following the San Francisco model, that 50% of visitors leave a review and the average number of nights per booking, where we use the min number of nights, again following the San Francisco model. For the number of people per listing we use the “accommodates” column.

**2.Average money spent locally by a tourist is assumed to be 30 GBP:** 

There are numerous sophisticated approaches to calculate the percentage of tourist expenditure benefiting the local economy, including using factors like length of stay and considering the multiplication effects of local spending. However, for the scope of this study we suggest to follow a more common sense led approach. According to a study by the GLA, the average visitor spending per day in London is 120 GBP in 2023. We assume that the majority of this amount will be spent on accommodation and non-local spending mainly on transport and tourist attractions. Local spending will likely include breakfast, perhaps dinner and shopping. So we assume it will be around 30 GBP or 25% of the total daily spending.

**3.Local hosts:**

We assume that there are two types of hosts:
(a) local hosts that have one listing and (b) non-local/professional hosts that have multiple listings. 

**This assuption really have something to support:**

Some Airbnb hosts have multiple listings.

A host may list separate rooms in the same apartment, or multiple apartments or homes available in their entirity.

Hosts with multiple listings are more likely to be running a business, are unlikely to be living in the property, and in violation of most short term rental laws designed to protect residential hou.inhttp://insideairbnb.com/london/

**4.Compare inner and outer**

For this study, inner London boroughs are the City of London, Camden, Hackney, Islington, Kensington & Chelsea, Lambeth, Southwark, Tower Hamlets, Wandsworth and Westminster. They are all in Transport zone 1 and hence well connected to the main tourist  sites. Outer London boroughs are all other boroughs.s.**

## 7.4 Equation

### 7.4.1 Money to the local economy for each listing (year 2023) = tourist local spending + income of single host (if host = local host).

**Income of single host =  price * occupancy rate * number of nights**

**1.1.0 First of all: Find out host == local host**

Based on **Assuption 3**:

Local hosts: We assume that there are two types of hosts:

(a) local hosts that have one listing and (b) non-local/professional hosts that have multiple listings. 

**Choose columns to filter:**

**Use column 'calculated_host_listings_count' to decide which host have more than 1 listing in Airbnb**

if a host 'calculated_host_listings_count' is more than 1, it means this host is a non-local hosts, which means this host probably won't have economic contribution to this community/borough.

The reason why we use column 'calculated_host_listings_count' is here:

https://rstudio-pubs-static.s3.amazonaws.com/407929_afc5ef0f2ad648389447a6ca3f4a7cd4.html

```{python}
listings_df1_localhost = listings_df1[listings_df1['calculated_host_listings_count'] == 1]
```

```{python}
listings_df1.shape
```

```{python}
listings_df1_localhost.shape
```

**This means almost half of host have multiple listings, which means half of hosts won't have economic contribution to this community/borough.**

This result is similar with the outcome from **Inside Airbnb**. It shows **43,382 (49.3%) single listings** and **44,565 (50.7%) multi-listings**

http://insideairbnb.com/london/




### Parameter 1.1.1: price

```{python}
#Just check
listings_df1_localhost.sample(5, random_state=42)[money]
```

**Check 'NA'**

```{python}
#Check 'NA'
listings_df1_localhost[listings_df1_localhost.listing_url.isna()][['id','listing_url','name','description','host_id','host_name','price']]
# No 'NA'
```

```{python}
#Just check again 
listings_df1_localhost.isnull().sum(axis=0).sort_values(ascending=False)[:20]
```

```{python}
# NAN of rows
listings_df1_localhost.isnull().sum(axis=1).sort_values(ascending=False).head(10)
```

### Parameter 1.1.2: occupancy rate

**This is highly related to 'review', which based on 'The Occupancy Model' of 'San Francisco Model'**

**Equation: Occupancy = review/(50%)**

```{python}
listings_df1_localhost['occupancy_rate'] = listings_df1_localhost['number_of_reviews_ltm'] / 12 * 3 / 0.50
print(listings_df1_localhost['occupancy_rate'])

print(listings_df1_localhost[['occupancy_rate']].sort_values(by='occupancy_rate', ascending=False)) # Just test
# Prints: [43380 rows x 1 columns]

# print(listings_df1_localhost[listings_df1['occupancy_rate'] == 0])
listings_df1_localhost[listings_df1_localhost['occupancy_rate'] == 0].shape 
# Prints: (25607, 36) 
# Which means (actually the whole 2023 year) almost half of listings don't be used
```

**There are 43380 listings in dataframe. However, there are 256078 rows which value of 'occupancy_rate' is 0.0. Which means these listings didn't get any review in 2023 year. Based on San F Model, it means almost half of listings don't be used in 2023 year.**

### Parameter 1.1.3: number of nights (per booking)

**Answer from ChatGPT**

I found some relevant information about Airbnb stays in London, but it doesn't directly specify the average length of stay for guests. However, the data provides insights into the typical booking patterns which may help infer the average stay length.

The data from Finder shows that a significant number of Airbnb listings in London have a minimum stay requirement of 1 or 2 nights, indicating a tendency for shorter stays. Specifically, 19,546 listings have a minimum stay of 1 night, and 17,062 listings have a minimum stay of 2 nights. This represents over half of all listings, suggesting that short-term stays are quite common in London​​**https://www.finder.com/uk/london-airbnb-statistics**
.

Additionally, Airbtics highlights that the type of property being rented on Airbnb can influence the length of stay. Entire homes and apartments, which constitute a significant portion of Airbnb listings in London, are often chosen for their comfort and amenities, potentially encouraging longer stays. Conversely, private rooms and shared spaces are commonly booked for overnight stays or shorter trip
**https://airbtics.com/average-airbnb-stay-length**s​​.

It's important to note that while this information gives an overview of booking trends in London, it doesn't provide a specific average stay length figure. For more precise and detailed data, you might want to explore dedicated Airbnb analytics platforms like Air
**https://www.airdna.co/vacation-rental-data/app/gb/london/london/overview**DNA​​.

**So I decided to use columns 'minimum_nights', 'minimum_minimum_nights' and 'maximum_minimum_nights' to analyse 'average number of nights per booking'**

'minimum_minimum_nights' and 'maximum_minimum_nights' describe adjustments made by **host** to the 'minimum_nights' in order to comply with customer demand as demand decreases or increases. For example, during certain public holiday periods when there are more tourists, **host** will consider increasing the 'minimum_nights' to make a profit; during certain time periods when there are fewer tourists, **host** will consider decreasing the 'minimum_nights' to promote consumption.

### 1.1.4 Calculating 'Income of single host'

```{python}
# 步骤 1: 创建新列，将 'price'、'Occupancy' 和 'minimum nights' 相乘
listings_df1_localhost['host_income'] = listings_df1_localhost['price'] * listings_df1_localhost['occupancy_rate'] * listings_df1_localhost['minimum_nights']

# 步骤 2: 计算总收入
host_income_quarter = listings_df1_localhost['host_income'].sum()

print(f"The host income for the quarter: ${host_income_quarter:.2f}")
```

## 1.2 Tourist local spending =  occupancy rate * number of nights * number of people * 30GBP 

### 1.2.0: First of all

**The items 'occupancy rate' and 'number of nights' is same as Equation1. But there is a huge difference here.**

**When we use Equation1, we filter the hosts who have multiple listings. However, tourists contribute to the local economy whether they are staying in the listings which belong to local host or a non-local host.**

**So this part, we use listings_df1**

### Parameter 1.2.1: occupancy rate

```{python}
listings_df1['occupancy_rate'] = listings_df1['number_of_reviews_ltm'] / 12 * 3 / 0.50
print(listings_df1['occupancy_rate'])

print(listings_df1[['occupancy_rate']].sort_values(by='occupancy_rate', ascending=False)) # Just test
# Prints: [87941 rows x 1 columns]

# print(listings_df1[listings_df1['occupancy_rate'] == 0])
listings_df1[listings_df1['occupancy_rate'] == 0].shape 
# Prints: (43248, 36) 
# Which means (actually the whole 2023 year) almost half of listings don't be used
```

### Parameter 1.2.2: number of nights (per booking)

**use column 'minimum_nights'**

### Parameter 1.2.3: number of people

**Let's consider 'number of people', which means number of tourist per listing. We consider 'accommodates' is equal to accommodates.**

### Parameter 1.2.4: each spending

**We don't have literatures to support our opinion. We use 30GBP temporarily.**

### 1.2.5 Calculating 'Tourist local spending'

```{python}
# 步骤 1: 创建新列，将 'price'、'Occupancy' 和 'minimum nights' 相乘
listings_df1['tourist_local_spending'] = listings_df1['occupancy_rate'] * listings_df1['minimum_nights'] * listings_df1['accommodates'] * 30

# 步骤 2: 计算总收入
tourist_local_spending_quarter = listings_df1['tourist_local_spending'].sum()

print(f"The host income for the quarter: ${tourist_local_spending_quarter:.2f}")
```

# Make all boroughs work

## Dataframe 'income_by_borough_df'

```{python}
import pandas as pd

# 假设df_2023_Season4_SC是你的原始DataFrame，并且它包含列'neighbourhood_cleansed', 'number_of_reviews_ltm', 'price', 和 'minimum_nights'

# 为了计算每个区域的入住率和房东收入，你可以使用以下函数
def calculate_host_income(df):
    df['occupancy_rate'] = df['number_of_reviews_ltm'] / 12 * 3 / 0.50
    df['host_income'] = df['price'] * df['occupancy_rate'] * df['minimum_nights']
    return df['host_income'].sum()

# 使用groupby方法按区域分组，并应用calculate_metrics函数
income_by_borough = listings_df1_localhost.groupby('neighbourhood_cleansed').apply(calculate_host_income)

# Convert the result to a DataFrame with the correct column name.
income_by_borough_df = income_by_borough.reset_index(name='income_by_borough')

print(type(income_by_borough_df))
income_by_borough_df
```

## Dataframe 'spending_by_borough_df'

```{python}
import pandas as pd

# The function to calculate tourist_local_spending.
def calculate_tourist_local_spending(df):
    df['occupancy_rate'] = df['number_of_reviews_ltm'] / 12 * 3 / 0.50
    df['tourist_local_spending'] = df['occupancy_rate'] * df['minimum_nights'] * df['accommodates'] * 30
    return df['tourist_local_spending'].sum()

# Applying the function to the grouped data.
spending_by_borough = listings_df1.groupby('neighbourhood_cleansed').apply(calculate_tourist_local_spending)

# Convert the result to a DataFrame with the correct column name.
spending_by_borough_df = spending_by_borough.reset_index(name='spending_by_borough')

print(type(spending_by_borough_df))
spending_by_borough_df
```

## Dataframe 'neighborhood_counts'

```{python}
import pandas as pd

# 假设 df 是你的 DataFrame，包含 neighbourhood_cleansed 列
# ...

# 使用 groupby() 和 size() 方法计算每个变量对应行的总数
neighborhood_counts = listings_df1.groupby('neighbourhood_cleansed').size().reset_index(name='total_listings')

# 或者使用 count() 方法，效果相同
# neighborhood_counts = df.groupby('neighbourhood_cleansed').count().reset_index()

# 打印结果

print(type(neighborhood_counts))
neighborhood_counts
```

## Merge these dataframes all together

```{python}
import pandas as pd

# Merging the dataframes on 'neighbourhood_cleansed'
merged_df = income_by_borough_df.merge(spending_by_borough_df, on='neighbourhood_cleansed')
merged_df = merged_df.merge(neighborhood_counts, on='neighbourhood_cleansed')
merged_df['contribution_per_listing'] = (merged_df['income_by_borough'] + merged_df['spending_by_borough']) / merged_df['total_listings']

merged_df
```

```{python}
# Merge your data with the GeoDataFrame
merged_gdf = gdf.merge(merged_df, left_on='NAME', right_on='neighbourhood_cleansed')

# Plot the map
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
merged_gdf.plot(column='contribution_per_listing', ax=ax, legend=True, cmap='RdBu_r', edgecolor='white',
                legend_kwds={'label': "Proportion by Borough",
                             'orientation': "vertical",
                             'shrink': 0.7})
plt.show()
```

```{python}
import matplotlib.pyplot as plt
import geopandas as gpd

# Assuming 'gdf' is your GeoDataFrame with the London borough boundaries
# and 'merged_df' is your DataFrame with the 'ratio' column.

# Merging the GeoDataFrame with your data
merged_gdf = gdf.merge(merged_df, left_on='NAME', right_on='neighbourhood_cleansed')

# Creating the plot
fig, ax = plt.subplots(1, 1, figsize=(15, 15))
merged_gdf.plot(column='contribution_per_listing', ax=ax, legend=True, cmap='RdBu_r', edgecolor='white',
                legend_kwds={'label': "Proportion by Borough",
                             'orientation': "vertical", 
                             'shrink': 0.7})

# Adding the borough names to the plot
for idx, row in merged_gdf.iterrows():
    plt.annotate(text=row['NAME'], xy=(row['geometry'].centroid.x, row['geometry'].centroid.y), ha='center', fontsize=8)

# Adding a title to the plot
plt.title('Economic Contribution in London Boroughs (per listing)', fontsize=20)

# Removing the axes for a cleaner look
ax.set_axis_off()

# Display the plot
plt.show()
```

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors

# Sorting the DataFrame by ratio in descending order for better visual representation
merged_sorted_df = merged_df.sort_values('contribution_per_listing', ascending=True)

# Set a color board
cmap = plt.get_cmap('RdBu_r')
norm = mcolors.Normalize(vmin=merged_sorted_df['contribution_per_listing'].min(), vmax=merged_sorted_df['contribution_per_listing'].max())
colors = cmap(norm(merged_sorted_df['contribution_per_listing']))

# Plotting the bar chart
fig, ax = plt.subplots(figsize=(10, 8))  # create a fig object and an ax object
ax.barh(merged_sorted_df['neighbourhood_cleansed'], merged_sorted_df['contribution_per_listing'], color=colors)
ax.set_xlabel('Contribution per Listing', fontsize=12)
ax.set_ylabel('Neighbourhood', fontsize=12)
ax.set_title('Economic Contribution in London Boroughs (per listing)', fontsize=20)

# Create a colorbar
sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
sm.set_array([])  # Just to show the colorbar
cbar = plt.colorbar(sm, ax=ax)  # define prarmeter of ax

plt.tight_layout()
plt.show()
```

